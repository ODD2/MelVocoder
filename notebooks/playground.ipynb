{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import librosa\n",
    "# import torchaudio\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "# from tqdm import tqdm\n",
    "# from librosa.feature.inverse import mel_to_audio\n",
    "# from preprocess import n_fft, hop_size, win_size, sampling_rate, fmin, fmax,num_mels,load_audio\n",
    "from IPython.display import Audio\n",
    "from meldataset import load_wav,MelDataset\n",
    "from utils import plot_spectrogram\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = MelDataset(\n",
    "    glob(\"dataset/audio/*.wav\"),\n",
    "    8192,\n",
    "    1024,\n",
    "    80,\n",
    "    256,\n",
    "    1024,\n",
    "    22050,\n",
    "    0,\n",
    "    8000,\n",
    "    shuffle=False,\n",
    "    n_cache_reuse=0,\n",
    "    device=None,\n",
    "    fmax_loss=8000,\n",
    "    fine_tuning=False,\n",
    "    split=True,\n",
    "    base_mels_path=\"dataset/mel/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(data[1].view(1, -1), rate=22050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spectrogram(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spectrogram(data[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thanks to the author providing the canny edge module: https://github.com/DCurro/CannyEdgePytorch\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from scipy.signal import gaussian\n",
    "\n",
    "\n",
    "class CannyEdge(nn.Module):\n",
    "    def __init__(self, threshold=8.0):\n",
    "        super(CannyEdge, self).__init__()\n",
    "\n",
    "        self.threshold = threshold\n",
    "\n",
    "        filter_size = 5\n",
    "        generated_filters = gaussian(filter_size, std=1.0).reshape([1, filter_size])\n",
    "\n",
    "        self.gaussian_filter_horizontal = nn.Conv2d(\n",
    "            in_channels=1, out_channels=1, kernel_size=(1, filter_size), padding=(0, filter_size // 2))\n",
    "        self.gaussian_filter_horizontal.weight.data.copy_(torch.from_numpy(generated_filters))\n",
    "        self.gaussian_filter_horizontal.bias.data.copy_(torch.from_numpy(np.array([0.0])))\n",
    "        self.gaussian_filter_vertical = nn.Conv2d(\n",
    "            in_channels=1, out_channels=1, kernel_size=(filter_size, 1), padding=(filter_size // 2, 0))\n",
    "        self.gaussian_filter_vertical.weight.data.copy_(torch.from_numpy(generated_filters.T))\n",
    "        self.gaussian_filter_vertical.bias.data.copy_(torch.from_numpy(np.array([0.0])))\n",
    "\n",
    "        sobel_filter = np.array([[1, 0, -1],\n",
    "                                 [2, 0, -2],\n",
    "                                 [1, 0, -1]])\n",
    "\n",
    "        self.sobel_filter_horizontal = nn.Conv2d(in_channels=1, out_channels=1,\n",
    "                                                 kernel_size=sobel_filter.shape, padding=sobel_filter.shape[0] // 2)\n",
    "        self.sobel_filter_horizontal.weight.data.copy_(torch.from_numpy(sobel_filter))\n",
    "        self.sobel_filter_horizontal.bias.data.copy_(torch.from_numpy(np.array([0.0])))\n",
    "        self.sobel_filter_vertical = nn.Conv2d(in_channels=1, out_channels=1,\n",
    "                                               kernel_size=sobel_filter.shape, padding=sobel_filter.shape[0] // 2)\n",
    "        self.sobel_filter_vertical.weight.data.copy_(torch.from_numpy(sobel_filter.T))\n",
    "        self.sobel_filter_vertical.bias.data.copy_(torch.from_numpy(np.array([0.0])))\n",
    "\n",
    "        # filters were flipped manually\n",
    "        filter_0 = np.array([[0, 0, 0],\n",
    "                             [0, 1, -1],\n",
    "                             [0, 0, 0]])\n",
    "\n",
    "        filter_45 = np.array([[0, 0, 0],\n",
    "                              [0, 1, 0],\n",
    "                              [0, 0, -1]])\n",
    "\n",
    "        filter_90 = np.array([[0, 0, 0],\n",
    "                              [0, 1, 0],\n",
    "                              [0, -1, 0]])\n",
    "\n",
    "        filter_135 = np.array([[0, 0, 0],\n",
    "                               [0, 1, 0],\n",
    "                               [-1, 0, 0]])\n",
    "\n",
    "        filter_180 = np.array([[0, 0, 0],\n",
    "                               [-1, 1, 0],\n",
    "                               [0, 0, 0]])\n",
    "\n",
    "        filter_225 = np.array([[-1, 0, 0],\n",
    "                               [0, 1, 0],\n",
    "                               [0, 0, 0]])\n",
    "\n",
    "        filter_270 = np.array([[0, -1, 0],\n",
    "                               [0, 1, 0],\n",
    "                               [0, 0, 0]])\n",
    "\n",
    "        filter_315 = np.array([[0, 0, -1],\n",
    "                               [0, 1, 0],\n",
    "                               [0, 0, 0]])\n",
    "\n",
    "        all_filters = np.stack([filter_0, filter_45, filter_90, filter_135,\n",
    "                               filter_180, filter_225, filter_270, filter_315])\n",
    "\n",
    "        self.directional_filter = nn.Conv2d(in_channels=1, out_channels=8,\n",
    "                                            kernel_size=filter_0.shape, padding=filter_0.shape[-1] // 2)\n",
    "        self.directional_filter.weight.data.copy_(torch.from_numpy(all_filters[:, None, ...]))\n",
    "        self.directional_filter.bias.data.copy_(torch.from_numpy(np.zeros(shape=(all_filters.shape[0],))))\n",
    "\n",
    "    def forward(self, img):\n",
    "        batch_size = img.shape[0]\n",
    "        img_r = img[:, 0:1]\n",
    "        img_g = img[:, 1:2]\n",
    "        img_b = img[:, 2:3]\n",
    "\n",
    "        blur_horizontal = self.gaussian_filter_horizontal(img_r)\n",
    "        blurred_img_r = self.gaussian_filter_vertical(blur_horizontal)\n",
    "        blur_horizontal = self.gaussian_filter_horizontal(img_g)\n",
    "        blurred_img_g = self.gaussian_filter_vertical(blur_horizontal)\n",
    "        blur_horizontal = self.gaussian_filter_horizontal(img_b)\n",
    "        blurred_img_b = self.gaussian_filter_vertical(blur_horizontal)\n",
    "\n",
    "        blurred_img = torch.stack([blurred_img_r, blurred_img_g, blurred_img_b], dim=1)\n",
    "        blurred_img = torch.stack([torch.squeeze(blurred_img)])\n",
    "\n",
    "        grad_x_r = self.sobel_filter_horizontal(blurred_img_r)\n",
    "        grad_y_r = self.sobel_filter_vertical(blurred_img_r)\n",
    "        grad_x_g = self.sobel_filter_horizontal(blurred_img_g)\n",
    "        grad_y_g = self.sobel_filter_vertical(blurred_img_g)\n",
    "        grad_x_b = self.sobel_filter_horizontal(blurred_img_b)\n",
    "        grad_y_b = self.sobel_filter_vertical(blurred_img_b)\n",
    "\n",
    "        # COMPUTE THICK EDGES\n",
    "\n",
    "        grad_mag = torch.sqrt(grad_x_r**2 + grad_y_r**2)\n",
    "        grad_mag += torch.sqrt(grad_x_g**2 + grad_y_g**2)\n",
    "        grad_mag += torch.sqrt(grad_x_b**2 + grad_y_b**2)\n",
    "        grad_orientation = (torch.atan2(grad_y_r + grad_y_g + grad_y_b,\n",
    "                            grad_x_r + grad_x_g + grad_x_b) * (180.0 / 3.14159))\n",
    "        grad_orientation += 180.0\n",
    "        grad_orientation = torch.round(grad_orientation / 45.0) * 45.0\n",
    "\n",
    "        # THIN EDGES (NON-MAX SUPPRESSION)\n",
    "\n",
    "        all_filtered = self.directional_filter(grad_mag)\n",
    "\n",
    "        inidices_positive = (grad_orientation / 45) % 8\n",
    "        inidices_negative = ((grad_orientation / 45) + 4) % 8\n",
    "\n",
    "        height = inidices_positive.size()[2]\n",
    "        width = inidices_positive.size()[3]\n",
    "        pixel_count = height * width\n",
    "        pixel_range = torch.tensor([range(pixel_count)], dtype=float).to(img.device)\n",
    "\n",
    "        indices = (inidices_positive.view(-1).data * pixel_count + pixel_range.repeat(1, batch_size)).squeeze()\n",
    "        channel_select_filtered_positive = all_filtered.view(-1)[indices.long()].view(batch_size, 1, height, width)\n",
    "\n",
    "        indices = (inidices_negative.view(-1).data * pixel_count + pixel_range.repeat(1, batch_size)).squeeze()\n",
    "        channel_select_filtered_negative = all_filtered.view(-1)[indices.long()].view(batch_size, 1, height, width)\n",
    "\n",
    "        channel_select_filtered = torch.cat([channel_select_filtered_positive, channel_select_filtered_negative], 1)\n",
    "        is_max = channel_select_filtered.min(dim=1)[0] > 0.0\n",
    "        is_max = torch.unsqueeze(is_max, dim=1)\n",
    "\n",
    "        thin_edges = grad_mag.clone()\n",
    "        thin_edges[is_max == 0] = 0.0\n",
    "\n",
    "        # THRESHOLD\n",
    "\n",
    "        thresholded = thin_edges.clone()\n",
    "        thresholded[thin_edges < self.threshold] = 0.0\n",
    "\n",
    "        early_threshold = grad_mag.clone()\n",
    "        early_threshold[grad_mag < self.threshold] = 0.0\n",
    "\n",
    "        assert grad_mag.size() == grad_orientation.size() == thin_edges.size() == thresholded.size() == early_threshold.size()\n",
    "\n",
    "        # return blurred_img, grad_mag, grad_orientation, thin_edges, thresholded, early_threshold\n",
    "        return thresholded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def canny(raw_img, use_cuda=False):\n",
    "    img = torch.from_numpy(raw_img.transpose((2, 0, 1)))\n",
    "    batch = torch.stack([img, img]).float()\n",
    "\n",
    "    net = CannyEdge(threshold=5)\n",
    "    if use_cuda:\n",
    "        net.cuda()\n",
    "    net.eval()\n",
    "\n",
    "    data = Variable(batch)\n",
    "    if use_cuda:\n",
    "        data = Variable(batch).cuda()\n",
    "\n",
    "    thresholded = net(data)\n",
    "    print(thresholded.shape)\n",
    "    cv2.imwrite('final.png', (thresholded.data.cpu().numpy()[1, 0] > 0.0).astype(float) * 255)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "img = librosa.power_to_db(np.load(\"dataset/mel/Alto-1#newboy#0000.npy\"))\n",
    "fig = plot_spectrogram(img)\n",
    "fig.show()\n",
    "img = (img - img.min()) / (img.max() - img.min())\n",
    "img = np.expand_dims(img, axis=-1).repeat(3, axis=-1)\n",
    "cv2.imwrite(\"img.png\", img * 255)\n",
    "\n",
    "canny(img, use_cuda=False)\n",
    "img = np.load(\"dataset/mel/Alto-1#newboy#0000.npy\")\n",
    "plot_spectrogram(librosa.power_to_db(img))\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.expand_dims(img, axis=0).repeat(3, axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Sobel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.filter = nn.Conv2d(in_channels=1, out_channels=2, kernel_size=3, stride=1, padding=0, bias=False)\n",
    "\n",
    "        Gx = torch.tensor([[2.0, 0.0, -2.0], [4.0, 0.0, -4.0], [2.0, 0.0, -2.0]]) * 0\n",
    "        Gy = torch.tensor([[4.0, 8.0, 4.0], [0.0, 0.0, 0.0], [-4.0, -8.0, -4.0]])\n",
    "        G = torch.cat([Gx.unsqueeze(0), Gy.unsqueeze(0)], 0)\n",
    "        G = G.unsqueeze(1)\n",
    "        self.filter.weight = nn.Parameter(G, requires_grad=False)\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.filter(img)\n",
    "        x = torch.mul(x, x)\n",
    "        x = torch.sum(x, dim=1, keepdim=True)\n",
    "        x = torch.sqrt(x + 1e-9)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectrogram(spectrogram):\n",
    "    fig, ax = plt.subplots(figsize=(10, 2))\n",
    "    im = ax.imshow(\n",
    "        spectrogram,\n",
    "        aspect=\"auto\",\n",
    "        origin=\"lower\",\n",
    "        interpolation='none'\n",
    "    )\n",
    "    plt.colorbar(im, ax=ax)\n",
    "\n",
    "    fig.canvas.draw()\n",
    "    plt.close()\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img = librosa.power_to_db(np.load(\"dataset/mel/Alto-1#newboy#0000.npy\"))\n",
    "img = np.expand_dims(np.expand_dims(img, axis=0), axis=0)\n",
    "sobel = Sobel()\n",
    "print(img.shape)\n",
    "result = sobel(torch.from_numpy(img))\n",
    "result = result / result.max()\n",
    "print(result.shape)\n",
    "plt.figure(figsize=(5, 5), layout=\"constrained\")\n",
    "plt.suptitle(\"Sobel-Filtered Mel-Spectrogram\")\n",
    "plt.imshow(\n",
    "    torch.pow(result[0, 0], 2),\n",
    "    aspect=\"auto\",\n",
    "    origin=\"lower\",\n",
    "    interpolation='none'\n",
    ")\n",
    "plt.savefig(\"sobel.png\")\n",
    "\n",
    "plt.figure(figsize=(5, 5), layout=\"constrained\")\n",
    "plt.suptitle(\"Original Mel-Spectrogram\")\n",
    "plt.imshow(\n",
    "    img[0, 0],\n",
    "    aspect=\"auto\",\n",
    "    origin=\"lower\",\n",
    "    interpolation='none'\n",
    ")\n",
    "plt.savefig(\"spectrogram.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_mel.max(), y_g_hat_mel.max())\n",
    "(y_mel.min(), y_g_hat_mel.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_mel = np.load(\"y_mel.npy\")\n",
    "y_g_hat_mel = np.load(\"y_g_hat_mel.npy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hifi-gan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
